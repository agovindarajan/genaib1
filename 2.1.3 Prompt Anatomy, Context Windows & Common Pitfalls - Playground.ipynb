{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.3 Prompt Anatomy, Context Windows & Common Pitfalls\n",
    "\n",
    "## Playground Notebook\n",
    "\n",
    "This notebook covers three foundational topics that will make you a better prompt engineer:\n",
    "\n",
    "| Topic | What You'll Learn |\n",
    "|-------|------------------|\n",
    "| **Anatomy of a Prompt** | The 5 building blocks of every effective prompt |\n",
    "| **Context Window Management** | How to work within token limits and manage long conversations |\n",
    "| **Common Pitfalls** | Mistakes that cause bad outputs — and how to fix them |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ============================================================\n",
    "#  CONFIGURATION - Change the model name here if needed\n",
    "# ============================================================\n",
    "MODEL = \"qwen2.5:1.5b\"  # Options: \"qwen2.5:1.5b\", \"llama3.2\", \"mistral\", \"gemma2\", etc.\n",
    "\n",
    "llm = ChatOllama(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def build_messages(message_dicts):\n",
    "    \"\"\"Convert a list of role/content dicts into LangChain message objects.\"\"\"\n",
    "    type_map = {\n",
    "        \"system\": SystemMessage,\n",
    "        \"user\": HumanMessage,\n",
    "        \"assistant\": AIMessage,\n",
    "    }\n",
    "    return [type_map[m[\"role\"]](content=m[\"content\"]) for m in message_dicts]\n",
    "\n",
    "\n",
    "def chat(messages, **kwargs):\n",
    "    \"\"\"Send messages to the model and return the response text.\"\"\"\n",
    "    _llm = ChatOllama(model=MODEL, **kwargs) if kwargs else llm\n",
    "    lc_messages = build_messages(messages)\n",
    "    start = time.time()\n",
    "    response = _llm.invoke(lc_messages)\n",
    "    elapsed = time.time() - start\n",
    "    content = response.content\n",
    "    display(Markdown(content))\n",
    "    print(f\"\\n\\u23f1\\ufe0f {elapsed:.2f}s | {len(content)} chars\")\n",
    "    return content\n",
    "\n",
    "\n",
    "def show_messages(messages):\n",
    "    \"\"\"Pretty-print the message list being sent to the model.\"\"\"\n",
    "    colors = {\"system\": \"#e74c3c\", \"user\": \"#3498db\", \"assistant\": \"#2ecc71\"}\n",
    "    html = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        color = colors.get(role, \"#888\")\n",
    "        content_preview = msg[\"content\"][:300] + (\"...\" if len(msg[\"content\"]) > 300 else \"\")\n",
    "        html += (\n",
    "            f'<div style=\"margin:6px 0;padding:8px 12px;border-left:4px solid {color};'\n",
    "            f'background:#1e1e1e;border-radius:4px;\">'\n",
    "            f'<strong style=\"color:{color};text-transform:uppercase;\">{role}</strong>'\n",
    "            f'<br><span style=\"color:#ccc;white-space:pre-wrap;\">{content_preview}</span></div>'\n",
    "        )\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "print(f\"\\u2705 Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Anatomy of an Effective Prompt\n",
    "\n",
    "Every well-crafted prompt is built from up to **5 components**. You don't always need all five, but knowing them helps you write better prompts every time.\n",
    "\n",
    "| Component | What It Does | Example |\n",
    "|-----------|-------------|--------|\n",
    "| **Task** | The core instruction — what you want the model to do | \"Summarize this article\" |\n",
    "| **Context** | Background information the model needs | \"This is a customer complaint about late delivery\" |\n",
    "| **Role** | Who the model should be (persona) | \"You are a senior data scientist\" |\n",
    "| **Format** | How the output should be structured | \"Respond as a numbered list\" |\n",
    "| **Constraints** | Boundaries and rules | \"Use 3 sentences max. No jargon.\" |\n",
    "\n",
    "```\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502  ROLE:        Who should the model be?       \\u2502\n",
    "\\u251c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\n",
    "\\u2502  CONTEXT:     What does it need to know?     \\u2502\n",
    "\\u251c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\n",
    "\\u2502  TASK:        What should it do?             \\u2502\n",
    "\\u251c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\n",
    "\\u2502  FORMAT:      How should it respond?         \\u2502\n",
    "\\u251c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\n",
    "\\u2502  CONSTRAINTS: What are the limits?           \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "```\n",
    "\n",
    "### Experiment 1A: Bare Prompt vs. Well-Structured Prompt\n",
    "\n",
    "Let's see the dramatic difference between a lazy prompt and one that uses all 5 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bare prompt: just the task, nothing else ---\n",
    "print(\"=\" * 60)\n",
    "print(\"BARE PROMPT (task only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bare = [\n",
    "    {\"role\": \"user\", \"content\": \"Write about machine learning.\"}\n",
    "]\n",
    "show_messages(bare)\n",
    "_ = chat(bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Well-structured prompt: all 5 components ---\n",
    "print(\"=\" * 60)\n",
    "print(\"WELL-STRUCTURED PROMPT (all 5 components)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "structured = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            # ROLE\n",
    "            \"You are a tech blogger who writes for beginners. \"\n",
    "            # CONSTRAINTS\n",
    "            \"Keep explanations simple. Avoid jargon. Use analogies from everyday life.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            # CONTEXT\n",
    "            \"I'm writing a blog post for people who have never heard of AI. \"\n",
    "            # TASK\n",
    "            \"Write an introduction paragraph explaining what machine learning is. \"\n",
    "            # FORMAT\n",
    "            \"Use exactly 3 sentences. \"\n",
    "            # CONSTRAINTS\n",
    "            \"Do not use the words 'algorithm' or 'neural network'.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "show_messages(structured)\n",
    "_ = chat(structured)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 Compare: The structured prompt gives a focused, audience-appropriate response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1B: Adding Components Incrementally\n",
    "\n",
    "Watch how the output improves as we add each component one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll build up the same prompt step by step\n",
    "levels = [\n",
    "    {\n",
    "        \"label\": \"Task only\",\n",
    "        \"system\": \"You are a helpful assistant.\",\n",
    "        \"user\": \"Explain diabetes.\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Task + Context\",\n",
    "        \"system\": \"You are a helpful assistant.\",\n",
    "        \"user\": \"I'm a newly diagnosed Type 2 diabetes patient. Explain diabetes.\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Task + Context + Role\",\n",
    "        \"system\": \"You are a compassionate family doctor speaking to a patient.\",\n",
    "        \"user\": \"I'm a newly diagnosed Type 2 diabetes patient. Explain diabetes.\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Task + Context + Role + Format\",\n",
    "        \"system\": \"You are a compassionate family doctor speaking to a patient.\",\n",
    "        \"user\": (\n",
    "            \"I'm a newly diagnosed Type 2 diabetes patient. Explain diabetes. \"\n",
    "            \"Structure your response as: 1) What it is, 2) Why it happens, 3) What I can do.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"All 5: Task + Context + Role + Format + Constraints\",\n",
    "        \"system\": (\n",
    "            \"You are a compassionate family doctor speaking to a patient. \"\n",
    "            \"Use simple language. No medical jargon. Keep it under 150 words.\"\n",
    "        ),\n",
    "        \"user\": (\n",
    "            \"I'm a newly diagnosed Type 2 diabetes patient. Explain diabetes. \"\n",
    "            \"Structure your response as: 1) What it is, 2) Why it happens, 3) What I can do.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "for level in levels:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  {level['label']}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": level[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": level[\"user\"]}\n",
    "    ]\n",
    "    show_messages(messages)\n",
    "    result = chat(messages, temperature=0.3)\n",
    "    print(f\"  [Word count: {len(result.split())}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1C: Structure and Clarity Matter\n",
    "\n",
    "Even with the same information, **how you organize** the prompt affects the output. Compare a messy prompt vs. a clearly structured one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Messy: all information crammed together ---\n",
    "print(\"=\" * 60)\n",
    "print(\"MESSY PROMPT (same info, poor structure)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "messy = [\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"I need you to act like a nutritionist and give me a meal plan \"\n",
    "        \"for someone who is vegetarian and wants to lose weight and \"\n",
    "        \"make it for one day with breakfast lunch dinner and snacks \"\n",
    "        \"and also mention calories for each meal and keep total under 1500 calories \"\n",
    "        \"and format it nicely\"\n",
    "    )}\n",
    "]\n",
    "show_messages(messy)\n",
    "_ = chat(messy, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean: same info, well-structured with clear sections ---\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEAN PROMPT (same info, clear structure)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clean = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a certified nutritionist. Provide evidence-based meal plans.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Create a one-day meal plan with the following requirements:\n",
    "\n",
    "**Patient profile:**\n",
    "- Vegetarian\n",
    "- Goal: weight loss\n",
    "\n",
    "**Meals to include:**\n",
    "- Breakfast, Lunch, Dinner, 2 Snacks\n",
    "\n",
    "**Constraints:**\n",
    "- Total calories: under 1500\n",
    "- Show calorie count for each meal\n",
    "\n",
    "**Format:**\n",
    "- Use a Markdown table with columns: Meal, Items, Calories\"\"\"\n",
    "    }\n",
    "]\n",
    "show_messages(clean)\n",
    "_ = chat(clean, temperature=0.3)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 Notice how the structured prompt gives a more organized, complete response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build Your Own Prompt\n",
    "\n",
    "Pick a task and build a prompt using all 5 components. Try:\n",
    "- **Scenario A**: A travel itinerary for 2 days in Chennai\n",
    "- **Scenario B**: A study plan for learning Python in 4 weeks\n",
    "- **Scenario C**: A product comparison for choosing a laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your prompt using all 5 components\n",
    "\n",
    "my_role = \"\"        # Who should the model be?\n",
    "my_constraints = \"\" # What limits or rules?\n",
    "my_context = \"\"     # What background info is needed?\n",
    "my_task = \"\"        # What should it do?\n",
    "my_format = \"\"      # How should it format the response?\n",
    "\n",
    "if my_task:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{my_role} {my_constraints}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{my_context} {my_task} {my_format}\"}\n",
    "    ]\n",
    "    show_messages(messages)\n",
    "    _ = chat(messages, temperature=0.5)\n",
    "else:\n",
    "    print(\"Fill in the 5 components above, then re-run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Context Window Management\n",
    "\n",
    "Every LLM has a **context window** — a maximum number of tokens it can process (input + output combined). Think of it as the model's \"working memory.\"\n",
    "\n",
    "| Concept | Explanation |\n",
    "|---------|------------|\n",
    "| **Token** | A piece of a word. Roughly: 1 token \\u2248 4 characters \\u2248 \\u00be of a word |\n",
    "| **Context window** | Total tokens the model can handle at once (input + output) |\n",
    "| **Input tokens** | System prompt + conversation history + user message |\n",
    "| **Output tokens** | The model's generated response |\n",
    "\n",
    "```\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500 Context Window (e.g., 4096 tokens) \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502  [System Prompt]  [Conversation History]  [User Msg]  [Response]  \\u2502\n",
    "\\u2502  \\u2190\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500 Input Tokens \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2192  \\u2190 Output \\u2192  \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "```\n",
    "\n",
    "**Common context window sizes:**\n",
    "\n",
    "| Model | Context Window |\n",
    "|-------|---------------|\n",
    "| GPT-3.5 | 4,096 \\u2013 16,384 tokens |\n",
    "| GPT-4o | 128,000 tokens |\n",
    "| Claude 3.5 Sonnet | 200,000 tokens |\n",
    "| Llama 3.2 | 128,000 tokens |\n",
    "| Qwen 2.5 (1.5B) | 32,768 tokens |\n",
    "\n",
    "### Experiment 2A: Understanding Token Counts\n",
    "\n",
    "Let's see how different texts translate into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Ollama doesn't expose a direct tokenizer, so we estimate:\n",
    "# Rule of thumb: 1 token ≈ 4 characters (English)\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Rough token estimate based on character count.\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "texts = [\n",
    "    (\"Short sentence\", \"Hello, how are you?\"),\n",
    "    (\"Medium paragraph\", \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.\"),\n",
    "    (\"Long passage\", \"\"\"The history of artificial intelligence dates back to the 1950s when pioneers like Alan Turing and John McCarthy laid the groundwork for what would become one of the most transformative fields in computer science. Turing proposed his famous test in 1950, asking whether machines could think. McCarthy coined the term 'artificial intelligence' at the Dartmouth Conference in 1956. Over the following decades, AI experienced cycles of optimism and disappointment known as 'AI winters.' The field saw renewed interest in the 2010s with the rise of deep learning, fueled by large datasets, powerful GPUs, and advances in neural network architectures. Today, AI powers everything from voice assistants to autonomous vehicles to medical diagnosis systems.\"\"\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Text':<20} {'Characters':<12} {'Est. Tokens':<12} {'Words':<8}\")\n",
    "print(\"-\" * 55)\n",
    "for label, text in texts:\n",
    "    chars = len(text)\n",
    "    tokens = estimate_tokens(text)\n",
    "    words = len(text.split())\n",
    "    print(f\"{label:<20} {chars:<12} {tokens:<12} {words:<8}\")\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 Rule of thumb: 1 token \\u2248 4 characters \\u2248 \\u00be of a word (in English)\")\n",
    "print(\"   A 4096-token window \\u2248 ~3000 words \\u2248 ~6 pages of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2B: What Happens When Context Gets Too Long\n",
    "\n",
    "When conversation history grows large, the model may:\n",
    "1. **Forget earlier context** — information at the beginning gets less attention\n",
    "2. **Produce lower quality responses** — too much noise in the input\n",
    "3. **Hit the limit and fail** — request gets rejected\n",
    "\n",
    "Let's simulate a long conversation and test if the model remembers early information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation where important info is mentioned early,\n",
    "# then buried under many turns of filler\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer questions based on the conversation.\"},\n",
    "    # Important info planted at the start\n",
    "    {\"role\": \"user\", \"content\": \"My name is Priya and I'm allergic to peanuts. Remember this.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Got it, Priya! I'll remember that you're allergic to peanuts.\"},\n",
    "]\n",
    "\n",
    "# Add filler conversation turns\n",
    "filler_topics = [\n",
    "    (\"What's the capital of France?\", \"The capital of France is Paris.\"),\n",
    "    (\"Tell me a fun fact about octopuses.\", \"Octopuses have three hearts and blue blood!\"),\n",
    "    (\"What's 17 times 23?\", \"17 times 23 equals 391.\"),\n",
    "    (\"Recommend a good book.\", \"I recommend 'Sapiens' by Yuval Noah Harari.\"),\n",
    "    (\"What causes rainbows?\", \"Rainbows are caused by light refracting through water droplets.\"),\n",
    "    (\"Name 3 programming languages.\", \"Python, JavaScript, and Rust are popular programming languages.\"),\n",
    "    (\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"),\n",
    "    (\"What's the tallest mountain?\", \"Mount Everest is the tallest mountain at 8,849 meters.\"),\n",
    "    (\"Explain photosynthesis briefly.\", \"Plants convert sunlight, water, and CO2 into glucose and oxygen.\"),\n",
    "    (\"What year did India gain independence?\", \"India gained independence in 1947.\"),\n",
    "]\n",
    "\n",
    "for user_msg, asst_msg in filler_topics:\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": asst_msg})\n",
    "\n",
    "# Now ask about the info from the beginning\n",
    "conversation.append({\"role\": \"user\", \"content\": \"What's my name and what am I allergic to?\"})\n",
    "\n",
    "print(f\"Total messages in conversation: {len(conversation)}\")\n",
    "total_chars = sum(len(m['content']) for m in conversation)\n",
    "print(f\"Estimated total tokens: ~{total_chars // 4}\")\n",
    "print(\"\\nAsking: 'What's my name and what am I allergic to?'\")\n",
    "print(\"=\" * 60)\n",
    "_ = chat(conversation, temperature=0.0)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 With a small context, the model should remember.\")\n",
    "print(\"   But as conversations grow to thousands of tokens, early info can get lost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2C: Strategy — Summarize Previous Context\n",
    "\n",
    "Instead of passing the entire conversation history, **summarize** older turns and keep only recent ones. This saves tokens while preserving important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Summarize old context into a compact system message\n",
    "\n",
    "# Full conversation approach (expensive)\n",
    "print(\"=\" * 60)\n",
    "print(\"APPROACH 1: Full conversation history (23 messages)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "full_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a project planning assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm building an e-commerce app using React and Node.js.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Great choice! React for the frontend and Node.js for the backend is a popular stack.\"},\n",
    "    {\"role\": \"user\", \"content\": \"The database will be PostgreSQL.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"PostgreSQL is excellent for e-commerce — it handles complex queries and transactions well.\"},\n",
    "    {\"role\": \"user\", \"content\": \"My team has 3 developers and we have 8 weeks.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"With 3 devs and 8 weeks, you'll want to prioritize core features first.\"},\n",
    "    {\"role\": \"user\", \"content\": \"The budget is $15,000.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"That's a reasonable budget for an MVP. Let's focus on essential features.\"},\n",
    "    # ... imagine many more turns ...\n",
    "    {\"role\": \"user\", \"content\": \"Give me a sprint plan for the first 2 weeks.\"}\n",
    "]\n",
    "\n",
    "total_chars = sum(len(m['content']) for m in full_conversation)\n",
    "print(f\"Estimated tokens: ~{total_chars // 4}\")\n",
    "_ = chat(full_conversation, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarized approach (token-efficient)\n",
    "print(\"=\" * 60)\n",
    "print(\"APPROACH 2: Summarized context (3 messages)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summarized_conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a project planning assistant. \"\n",
    "            \"Here is a summary of the conversation so far: \"\n",
    "            \"The user is building an e-commerce app with React (frontend), \"\n",
    "            \"Node.js (backend), and PostgreSQL (database). \"\n",
    "            \"Team: 3 developers. Timeline: 8 weeks. Budget: $15,000.\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Give me a sprint plan for the first 2 weeks.\"}\n",
    "]\n",
    "\n",
    "total_chars = sum(len(m['content']) for m in summarized_conversation)\n",
    "print(f\"Estimated tokens: ~{total_chars // 4}\")\n",
    "_ = chat(summarized_conversation, temperature=0.3)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 The summarized version uses far fewer tokens but preserves key facts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2D: Sliding Window — Keep Only Last N Turns\n",
    "\n",
    "Another strategy: only keep the **most recent N message pairs** in the conversation. Older messages get dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(messages, max_pairs=3):\n",
    "    \"\"\"\n",
    "    Keep the system message + only the last N user-assistant pairs.\n",
    "    \"\"\"\n",
    "    system_msgs = [m for m in messages if m[\"role\"] == \"system\"]\n",
    "    non_system = [m for m in messages if m[\"role\"] != \"system\"]\n",
    "    # Keep last max_pairs * 2 messages (each pair = user + assistant)\n",
    "    trimmed = non_system[-(max_pairs * 2):]\n",
    "    return system_msgs + trimmed\n",
    "\n",
    "\n",
    "# Simulate a long conversation\n",
    "long_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I create a list in Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use square brackets: my_list = [1, 2, 3]\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I add an item?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use .append(): my_list.append(4)\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I remove an item?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use .remove(value) or .pop(index)\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I sort the list?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use .sort() for in-place or sorted() for a new list\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I reverse it?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use .reverse() or slicing: my_list[::-1]\"},\n",
    "    # New question\n",
    "    {\"role\": \"user\", \"content\": \"Now how do I find the length?\"}\n",
    "]\n",
    "\n",
    "print(\"FULL CONVERSATION\")\n",
    "print(f\"Messages: {len(long_conversation)}\")\n",
    "print()\n",
    "\n",
    "# Apply sliding window\n",
    "windowed = sliding_window(long_conversation, max_pairs=2)\n",
    "\n",
    "print(\"AFTER SLIDING WINDOW (keep last 2 pairs)\")\n",
    "print(f\"Messages: {len(windowed)}\")\n",
    "print(\"-\" * 40)\n",
    "for m in windowed:\n",
    "    print(f\"  [{m['role']:>9}] {m['content'][:60]}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"Response with windowed context:\")\n",
    "_ = chat(windowed, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Window Management — Summary of Strategies\n",
    "\n",
    "| Strategy | How It Works | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Full history** | Pass everything | Complete context | Expensive, may hit limits |\n",
    "| **Sliding window** | Keep last N turns | Simple, predictable | Loses old info |\n",
    "| **Summarization** | Summarize old turns | Compact, preserves key facts | Requires extra LLM call |\n",
    "| **Hybrid** | Summarize old + keep recent | Best of both | More complex to implement |\n",
    "\n",
    "### Exercise 2: Token Budget Calculator\n",
    "\n",
    "Calculate how many conversation turns you can fit in different context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many turns fit in a context window?\n",
    "\n",
    "system_prompt_tokens = 50     # Typical system prompt\n",
    "avg_user_tokens = 30          # Average user message\n",
    "avg_assistant_tokens = 150    # Average assistant response\n",
    "output_reserve = 500          # Tokens reserved for the next response\n",
    "\n",
    "tokens_per_turn = avg_user_tokens + avg_assistant_tokens  # One turn = user + assistant\n",
    "\n",
    "context_windows = [4096, 8192, 32768, 128000]\n",
    "\n",
    "print(f\"Assumptions:\")\n",
    "print(f\"  System prompt: ~{system_prompt_tokens} tokens\")\n",
    "print(f\"  Avg user msg:  ~{avg_user_tokens} tokens\")\n",
    "print(f\"  Avg assistant: ~{avg_assistant_tokens} tokens\")\n",
    "print(f\"  Output reserve: {output_reserve} tokens\")\n",
    "print(f\"  Per turn:      ~{tokens_per_turn} tokens\")\n",
    "print()\n",
    "print(f\"{'Context Window':<18} {'Available':<12} {'Max Turns':<12}\")\n",
    "print(\"-\" * 42)\n",
    "for window in context_windows:\n",
    "    available = window - system_prompt_tokens - output_reserve\n",
    "    max_turns = available // tokens_per_turn\n",
    "    print(f\"{window:>10} tokens  {available:>8}       {max_turns:>5} turns\")\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 This is why context management matters — even 128K windows fill up in long sessions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Common Prompt Pitfalls\n",
    "\n",
    "Even experienced developers write bad prompts sometimes. Here are the most common mistakes and how to fix them.\n",
    "\n",
    "| Pitfall | Problem | Fix |\n",
    "|---------|---------|-----|\n",
    "| **Vague instructions** | Model guesses what you want | Be specific about task, format, and length |\n",
    "| **Overloaded prompts** | Too many tasks in one prompt | Break into separate prompts |\n",
    "| **Leading/biased phrasing** | Model follows your bias instead of being objective | Use neutral language |\n",
    "| **Missing constraints** | Model rambles or adds unwanted content | Set explicit boundaries |\n",
    "| **Conflicting instructions** | System and user prompts contradict | Align system and user messages |\n",
    "\n",
    "### Experiment 3A: Vague Instructions \\u2192 Inconsistent Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same vague prompt 3 times — watch how different the outputs are\n",
    "print(\"=\" * 60)\n",
    "print(\"PITFALL: Vague instructions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vague_prompt = [{\"role\": \"user\", \"content\": \"Write something about dogs.\"}]\n",
    "\n",
    "print(\"Prompt: 'Write something about dogs.'\\n\")\n",
    "for run in range(3):\n",
    "    print(f\"--- Run {run + 1} ---\")\n",
    "    result = chat(vague_prompt, temperature=0.8)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fixed version ---\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXED: Specific instructions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "specific_prompt = [\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Write 3 fun facts about golden retrievers. \"\n",
    "        \"Use a numbered list. One sentence per fact.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "print(\"Prompt: 'Write 3 fun facts about golden retrievers...'\\n\")\n",
    "for run in range(3):\n",
    "    print(f\"--- Run {run + 1} ---\")\n",
    "    result = chat(specific_prompt, temperature=0.8)\n",
    "    print()\n",
    "\n",
    "print(\"\\ud83d\\udca1 Much more consistent! Specificity reduces variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3B: Overloaded Prompts \\u2192 Incomplete Output\n",
    "\n",
    "Asking the model to do too many things in one prompt often leads to some tasks being done poorly or skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Overloaded: 5 tasks crammed into one prompt ---\n",
    "print(\"=\" * 60)\n",
    "print(\"PITFALL: Overloaded prompt (too many tasks)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "overloaded = [\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Explain what a REST API is, give me a Python code example, \"\n",
    "        \"list the HTTP methods, compare REST vs GraphQL, \"\n",
    "        \"and suggest when to use each one.\"\n",
    "    )}\n",
    "]\n",
    "show_messages(overloaded)\n",
    "_ = chat(overloaded, temperature=0.3, num_predict=300)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 Notice: With a token limit, some tasks get skipped or rushed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fixed: Break into focused prompts ---\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXED: One task at a time\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "focused_tasks = [\n",
    "    \"Explain what a REST API is in 2-3 sentences.\",\n",
    "    \"List the 5 main HTTP methods with a one-line description each.\",\n",
    "    \"Compare REST vs GraphQL in a 3-row table (columns: Feature, REST, GraphQL).\",\n",
    "]\n",
    "\n",
    "for i, task in enumerate(focused_tasks, 1):\n",
    "    print(f\"\\n--- Task {i} ---\")\n",
    "    messages = [{\"role\": \"user\", \"content\": task}]\n",
    "    _ = chat(messages, temperature=0.3)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 Each response is now complete and focused!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3C: Leading / Biased Prompts\n",
    "\n",
    "The way you phrase a question can **bias** the model's response. This is especially dangerous for tasks requiring objectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = {\"role\": \"system\", \"content\": \"You are an objective technology analyst. Give balanced assessments.\"}\n",
    "\n",
    "prompts = [\n",
    "    {\n",
    "        \"label\": \"BIASED (leading question)\",\n",
    "        \"content\": \"Why is Python the best programming language for everything?\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"NEUTRAL (balanced question)\",\n",
    "        \"content\": \"What are the strengths and weaknesses of Python compared to other languages?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  {p['label']}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    messages = [system, {\"role\": \"user\", \"content\": p[\"content\"]}]\n",
    "    show_messages(messages)\n",
    "    _ = chat(messages, temperature=0.3, num_predict=250)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 The biased prompt pushes the model to agree. Neutral phrasing gets balanced analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3D: Missing Constraints \\u2192 Model Rambles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- No constraints: model decides everything ---\n",
    "print(\"=\" * 60)\n",
    "print(\"PITFALL: No constraints\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "no_constraints = [{\"role\": \"user\", \"content\": \"Tell me about climate change.\"}]\n",
    "result_nc = chat(no_constraints, temperature=0.5)\n",
    "print(f\"\\n  [Word count: {len(result_nc.split())}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- With constraints: focused and controlled ---\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXED: Clear constraints\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with_constraints = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a science communicator. Keep responses concise and factual.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Explain the top 3 causes of climate change. \"\n",
    "            \"Use a numbered list. One sentence per cause. \"\n",
    "            \"No more than 50 words total.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "result_wc = chat(with_constraints, temperature=0.3)\n",
    "print(f\"\\n  [Word count: {len(result_wc.split())}]\")\n",
    "\n",
    "print(f\"\\n\\ud83d\\udca1 Unconstrained: {len(result_nc.split())} words vs. Constrained: {len(result_wc.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3E: Conflicting Instructions\n",
    "\n",
    "What happens when the system prompt says one thing and the user prompt says another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conflicting: system says formal, user says casual ---\n",
    "print(\"=\" * 60)\n",
    "print(\"PITFALL: Conflicting instructions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "conflicting = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a formal academic professor. Always use scholarly language and cite sources.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hey bro, explain quantum physics like I'm 5. Keep it super chill and funny. Use slang.\"\n",
    "    }\n",
    "]\n",
    "show_messages(conflicting)\n",
    "_ = chat(conflicting, temperature=0.5)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 The model is confused \\u2014 it tries to satisfy both, resulting in an awkward mix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fixed: aligned instructions ---\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXED: Aligned instructions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "aligned = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a fun, casual science explainer. Use simple language and humor.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain quantum physics like I'm 5. Keep it super chill and funny.\"\n",
    "    }\n",
    "]\n",
    "show_messages(aligned)\n",
    "_ = chat(aligned, temperature=0.5)\n",
    "\n",
    "print(\"\\n\\ud83d\\udca1 When system and user are aligned, the output is coherent and on-tone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fix the Broken Prompts\n",
    "\n",
    "Each prompt below has a problem. Run it, identify the issue, then fix it in the cell after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Broken Prompt 1: Too vague ---\n",
    "print(\"BROKEN PROMPT 1\")\n",
    "print(\"=\" * 40)\n",
    "broken_1 = [{\"role\": \"user\", \"content\": \"Help me with my project.\"}]\n",
    "_ = chat(broken_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix broken prompt 1 here:\n",
    "fixed_1 = [{\"role\": \"user\", \"content\": \"\"}]  # <-- Write your fix\n",
    "\n",
    "if fixed_1[0][\"content\"]:\n",
    "    print(\"YOUR FIX:\")\n",
    "    _ = chat(fixed_1)\n",
    "else:\n",
    "    print(\"Write your improved prompt above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Broken Prompt 2: Conflicting tone ---\n",
    "print(\"BROKEN PROMPT 2\")\n",
    "print(\"=\" * 40)\n",
    "broken_2 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a children's storyteller. Use fun, playful language.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a technical analysis of TCP/IP networking protocols with RFC references.\"}\n",
    "]\n",
    "_ = chat(broken_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix broken prompt 2 here:\n",
    "fixed_2 = [\n",
    "    {\"role\": \"system\", \"content\": \"\"},  # <-- Fix the system prompt\n",
    "    {\"role\": \"user\", \"content\": \"\"}      # <-- Fix the user prompt\n",
    "]\n",
    "\n",
    "if fixed_2[0][\"content\"] and fixed_2[1][\"content\"]:\n",
    "    print(\"YOUR FIX:\")\n",
    "    _ = chat(fixed_2)\n",
    "else:\n",
    "    print(\"Write your improved prompts above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Broken Prompt 3: No format, no constraints ---\n",
    "print(\"BROKEN PROMPT 3\")\n",
    "print(\"=\" * 40)\n",
    "broken_3 = [{\"role\": \"user\", \"content\": \"Compare React and Angular.\"}]\n",
    "_ = chat(broken_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix broken prompt 3 here:\n",
    "fixed_3 = [{\"role\": \"user\", \"content\": \"\"}]  # <-- Add format and constraints\n",
    "\n",
    "if fixed_3[0][\"content\"]:\n",
    "    print(\"YOUR FIX:\")\n",
    "    _ = chat(fixed_3)\n",
    "else:\n",
    "    print(\"Write your improved prompt above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Prompt Anatomy\n",
    "\n",
    "| Component | Purpose | Impact on Output |\n",
    "|-----------|---------|------------------|\n",
    "| **Task** | What to do | Determines the core response |\n",
    "| **Context** | Background info | Makes response relevant |\n",
    "| **Role** | Who to be | Shapes tone and expertise level |\n",
    "| **Format** | How to structure | Controls output layout |\n",
    "| **Constraints** | What limits to follow | Prevents rambling and off-topic content |\n",
    "\n",
    "### Context Window Management\n",
    "\n",
    "| Strategy | When to Use |\n",
    "|----------|-------------|\n",
    "| **Full history** | Short conversations (< 10 turns) |\n",
    "| **Sliding window** | Chatbots where recent context matters most |\n",
    "| **Summarization** | Long sessions where early facts are important |\n",
    "| **Token budgeting** | Production systems with cost constraints |\n",
    "\n",
    "### Common Pitfalls Cheat Sheet\n",
    "\n",
    "| Pitfall | Quick Fix |\n",
    "|---------|----------|\n",
    "| Vague instructions | Add task + format + constraints |\n",
    "| Overloaded prompt | One task per prompt |\n",
    "| Biased/leading phrasing | Use neutral \"what are the pros and cons\" |\n",
    "| No constraints | Set length, format, and scope limits |\n",
    "| Conflicting instructions | Align system and user prompts |\n",
    "\n",
    "---\n",
    "\n",
    "*Nunnari Academy | Generative AI & Agentic AI Professional Program*  \n",
    "*Module 1, Week 2, Section 2.1 | Prompt Anatomy, Context Windows & Common Pitfalls*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
